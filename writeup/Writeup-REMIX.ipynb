{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 445 Final Project : Reinforcement Learning with Flappy Bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Cole Juracek. Credit to Chuck Anderson for the general algorithm and information from lecture slides, as well as Stack Overflow for general Python/Pygame questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTE\n",
    "\n",
    "I cannot seem to get the game to display within the notebook. All of the code for the game is provided, however, and can be run outside of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general goal of this project is two-fold:\n",
    "1. Code a Flappy Bird type game through Python\n",
    "2. Control the box's movement through reinforcment learning to teach it to go through the gap in the walls.\n",
    "    \n",
    "I have always wanted to code some sort of game in Python, and Pygame was the natural choice. It's fairly intuitive, and didn't take as long as I would have thought to code the game. In addition to this, I've thought the concept of reinforcement learning was very interesting. So to learn how to play a game based off nothing more than states, actions, and reinforcements seemed like a great choice for a final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game itself will be familiar to anyone who has played Flappy Bird before. The goal is to get the box through the gap between the two scrolling walls. Collision with either the walls or the floor will result in a game over.\n",
    "\n",
    "Implementation of the game was fairly simple, although it took a little while to learn Pygame's API. First, the game is initialized and the boilerplate code is handled. The game is advanced frame-by-frame at a rate of 30FPS. At each frame, the walls scroll forward, and new walls are initialized when these reach the end of the screen. The box's motion is handled by `box.py`, which uses equations for projectile motion to handle gravity. If at any point the space bar is pressed, the box will \"jump\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Again, the game will run without being displayed. Because of this, it will \"crash\" almost immediately due to falling down and hitting the floor. Copy out of the notebook to actually play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pygame\n",
    "\n",
    "from flappy_bird import box, game_constants\n",
    "\n",
    "def play_game():\n",
    "\n",
    "    pygame.init()\n",
    "\n",
    "    game_display = pygame.display.set_mode((game_constants.DISPLAY_WIDTH, game_constants.DISPLAY_HEIGHT))\n",
    "    pygame.display.set_caption('Flappy Bird')\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    flappy_box = box.Box(game_constants.DISPLAY_WIDTH * 0.25, game_constants.DISPLAY_HEIGHT * 0.25)\n",
    "\n",
    "    delta_x = -15\n",
    "    wall_x_positions = [x for x in range(game_constants.DISPLAY_WIDTH, 0, delta_x)]\n",
    "    current_wall_position = 0\n",
    "    top_wall_lengths = [x for x in range(50, 260, 30)]\n",
    "    bottom_wall_lengths = [x for x in range(260, 50, -30)]\n",
    "    index = random.randint(0, len(top_wall_lengths) - 1)\n",
    "    top_wall_length, bottom_wall_length = top_wall_lengths[index], bottom_wall_lengths[index]\n",
    "\n",
    "    score = 0\n",
    "    while True:\n",
    "        for event in pygame.event.get():\n",
    "            # If space down, make the box jump\n",
    "            if event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE:\n",
    "                flappy_box.jump()\n",
    "\n",
    "        game_display.fill(game_constants.WHITE)\n",
    "\n",
    "        # Draw the floor\n",
    "        floor = pygame.draw.rect(game_display, game_constants.BLACK, pygame.Rect((0, game_constants.DISPLAY_HEIGHT * 0.92),\n",
    "                                                                  (game_constants.DISPLAY_WIDTH, 50)))\n",
    "\n",
    "        # Draw the walls\n",
    "        top_wall = pygame.Rect((wall_x_positions[current_wall_position], 0), (20, top_wall_length))\n",
    "        top_wall = pygame.draw.rect(game_display, game_constants.BLACK, top_wall)\n",
    "\n",
    "        bottom_wall_start = game_constants.DISPLAY_HEIGHT - floor.height - bottom_wall_length  # Top of bottom wall\n",
    "        bottom_wall = pygame.Rect((wall_x_positions[current_wall_position], bottom_wall_start), (20, bottom_wall_length))\n",
    "        bottom_wall = pygame.draw.rect(game_display, game_constants.BLACK, bottom_wall)\n",
    "\n",
    "        # Update flappy box\n",
    "        flappy_box.update_position(game_display)\n",
    "        flappy_box.increment_time()\n",
    "\n",
    "        # Check for collision with floor + walls\n",
    "        collision_surfaces = [floor, top_wall, bottom_wall]\n",
    "        for collision_surface in collision_surfaces:\n",
    "            if flappy_box.rect.colliderect(collision_surface):\n",
    "                pygame.quit()\n",
    "                print(f'GAME OVER. SCORE: {score}')\n",
    "\n",
    "        # If walls at end of screen, create new walls\n",
    "        current_wall_position += 1\n",
    "        if current_wall_position > len(wall_x_positions) - 1:\n",
    "            index = random.randint(0, len(top_wall_lengths) - 1)\n",
    "            top_wall_length, bottom_wall_length = top_wall_lengths[index], bottom_wall_lengths[index]\n",
    "\n",
    "        current_wall_position = current_wall_position % len(wall_x_positions)\n",
    "\n",
    "        # Update graphics + advance 1 frame\n",
    "        pygame.display.update()\n",
    "        clock.tick(game_constants.FRAMES_PER_SECOND)\n",
    "        score += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME OVER. SCORE: 39\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "video system not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m, in \u001b[0;36mplay_game\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m current_wall_position \u001b[38;5;241m=\u001b[39m current_wall_position \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(wall_x_positions)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Update graphics + advance 1 frame\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m clock\u001b[38;5;241m.\u001b[39mtick(game_constants\u001b[38;5;241m.\u001b[39mFRAMES_PER_SECOND)\n\u001b[1;32m     67\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31merror\u001b[0m: video system not initialized"
     ]
    }
   ],
   "source": [
    "play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief note: there will be some relaxed constraints on the game such that the reinforcement learning problem is simpler. Namely, the walls will not appear in random positions, but rather in one fixed position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general premise of reinforcement learning is to \"reward\" an agent with reinforcements for positive moves toward a goal. This is accomplished via 3 key components:\n",
    "1. *state*: any information necessary to represent the agent's current state\n",
    "2. *actions*: the set of available actions for the agent in a given state\n",
    "3. *reinforcement*: represents how well received the current (state, action) pair is\n",
    "\n",
    "Ideally, the agent would like to pick the action from a given state with the highest reinforcement. But not only should the agent account for the immediate future, it should also account for future states down the line. Because of this, the agent should pick a move based off of the summation of all returns from the future states. This function that predicts the sum of future reinforcements is known as the *Q-function*, and utilizing it in reinforcement learning is known as *Q-Learning*. The general idea is for the agent to examine all of the Q-values for its (state, action) pairs, and pick the move with the highest value.\n",
    "\n",
    "We would like to update the Q-values to be more accurate over time as the agent explores the problem. There are two main approaches to this. The first is known as *Monte-Carlo*, where Q(state, action) is updated through the average over many trials. Another approach, and the one used for this project, updates Q(state, action) via the next state, action, and reinforcement in a process known as *temporal difference*:\n",
    "\n",
    "Finally, we run into the dilema of exploration vs. exploitation in reinforcement learning. This problem is stated as the following: should the agent attempt to learn as much as possible about the environment to fill its Q-table (exploration)? Or should the agent only pick the \"greedy\" action (the one with the highest Q-value) from each state (exploitation)? This issue is resolved by the epsilon parameter. During each iteration, decay epsilon by multiplying it by some fraction (epsilon \"decay\"). The lower epsilon becomes, the more likely the agent is to pick the greedy option. In effect, this will cause the agent to move from a policy of exploration toward exploitation over the course of its journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Applied to Flappy Bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the game of flappy bird, we will define the state as the y-position, and the actions as {0, 1}, corresponding to not jumping and jumping respectively. The reinforcement will more or less be negligible except for the case when the box is colliding with a wall or floor. In this case, the reinforcement for the (state, action) pair will be assigned as low as possible; this corresponds to a game over and is the only real issue we're trying to avoid. If we note the agent is in the gap between the two walls, we will also provide a *positive* reinforcement.\n",
    "\n",
    "There is no end state in this game; collisions with the floor and walls will not stop it. Each frame will be considered a new \"iteration\", and the box will move from exploration to exploitation over the course of around 1 minute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default parameters to the game are as follows:\n",
    "- *epsilon*: 1.0 (standard)\n",
    "- *epsilon_decay*: 0.999\n",
    "    - While this might seem like a high value, it is being applied every frame, so epsilon will still decay in a reasonable time.\n",
    "    \n",
    "    \n",
    "- *learning_rate*: 0.8\n",
    "    - This is how much \"weight\" we should give to the previous Q-value based off the current temporal difference formula. A value of 1.0 will simply be the formula itself. We will choose a default of slightly less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Games will be run for a length of two minutes, and the position of the box will be recorded on each frame. There is no means for the game stopping, so interrupting the kernel is necessary; the results will still be saved in 'y_positions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pygame\n",
    "import sys\n",
    "\n",
    "from flappy_bird import box, game_constants\n",
    "\n",
    "moves = []\n",
    "y_positions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_game(epsilon_decay=0.999, learning_rate=0.8, reinf_function=None):\n",
    "    pygame.init()\n",
    "\n",
    "    game_display = pygame.display.set_mode((game_constants.DISPLAY_WIDTH, game_constants.DISPLAY_HEIGHT))\n",
    "    pygame.display.set_caption('Flappy Bird')\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    flappy_box = box.Box(game_constants.DISPLAY_WIDTH * 0.25, game_constants.DISPLAY_HEIGHT * 0.5)\n",
    "\n",
    "    delta_x = -15\n",
    "    wall_x_positions = [x for x in range(game_constants.DISPLAY_WIDTH, 0, delta_x)]\n",
    "    current_wall_position = 0\n",
    "\n",
    "    Q = {}  # Lookup table for Q-values\n",
    "    epsilon = 1.0\n",
    "    valid_moves = [0, 1]  # Where 0 = Do nothing\n",
    "                          #       1 = Jump\n",
    "    state = flappy_box.y_pos\n",
    "\n",
    "    # Return a new move\n",
    "    def make_move(state, move):\n",
    "        if move == 1:\n",
    "            flappy_box.jump()\n",
    "\n",
    "        # Update flappy box\n",
    "        flappy_box.update_position(game_display)\n",
    "        flappy_box.increment_time()\n",
    "\n",
    "        return flappy_box.y_pos\n",
    "\n",
    "    num_moves = 0\n",
    "    epsilon_counter = 0\n",
    "    \n",
    "    while True:\n",
    "        for event in pygame.event.get():\n",
    "            x = 4\n",
    "\n",
    "        game_display.fill(game_constants.WHITE)\n",
    "        # Draw the floor\n",
    "        floor = pygame.draw.rect(game_display, game_constants.BLACK, pygame.Rect((0, game_constants.DISPLAY_HEIGHT * 0.92),\n",
    "                                                                  (game_constants.DISPLAY_WIDTH, 50)))\n",
    "        # Draw the walls\n",
    "        top_wall = pygame.Rect((wall_x_positions[current_wall_position], 0), (20, 150))\n",
    "        top_wall = pygame.draw.rect(game_display, game_constants.BLACK, top_wall)\n",
    "        bottom_wall = pygame.Rect((wall_x_positions[current_wall_position], 350), (20, 250))\n",
    "        bottom_wall = pygame.draw.rect(game_display, game_constants.BLACK, bottom_wall)\n",
    "\n",
    "        # TODO: Consider default value; might want to make very positive vs. very negative\n",
    "        # Pick a move\n",
    "        epsilon *= epsilon_decay\n",
    "        if random.uniform(0, 1) < epsilon:  # Random move\n",
    "            move = random.choice(valid_moves)\n",
    "        else:  # Greedy move\n",
    "            q_values = [Q.get((state, action), -100) for action in valid_moves]\n",
    "            move = valid_moves[q_values.index(max(q_values))]\n",
    "\n",
    "        new_state = make_move(state, move)\n",
    "\n",
    "        # TODO: Consider different default value\n",
    "        # If (state, move) tuple does not exist, add it to Q-table\n",
    "        if (state, move) not in Q:\n",
    "            Q[(state, move)] = -100\n",
    "\n",
    "        # Check for collision with floor + walls\n",
    "        # If there is a collision, we should assign with as much negative reinforcement as possible, as this would\n",
    "        #   be a game over.\n",
    "        collision_surfaces = [floor, top_wall, bottom_wall]\n",
    "        if any([flappy_box.rect.colliderect(surface) for surface in collision_surfaces])\n",
    "            Q[(state, move)] = -sys.maxsize\n",
    "        # elif 200 <= state <= 300 and -10 <= flappy_box.y_velocity <= 10:\n",
    "            # Q[(state, move)] = sys.maxsize\n",
    "        # elif state == 0:\n",
    "            # Q[(state, move)] = -sys.maxsize\n",
    "        elif 225 <= state <= 275 and 175 <= top_wall.center[0] <= 225:\n",
    "            Q[(state, move)] = 100\n",
    "\n",
    "        # If walls at end of screen, create new walls\n",
    "        current_wall_position += 1\n",
    "        current_wall_position = current_wall_position % len(wall_x_positions)\n",
    "\n",
    "        # Update graphics + advance 1 frame\n",
    "        pygame.display.update()\n",
    "        clock.tick(game_constants.FRAMES_PER_SECOND)\n",
    "\n",
    "        # TODO: Consider adding reinforcement\n",
    "        # Update Q-table of the old (state, move) tuple by calculating the temporal difference error.\n",
    "        # Reinforcement of -1\n",
    "        if num_moves > 1 and reinf_function is None:\n",
    "            Q[(old_state, old_move)] += (learning_rate * (Q[state, move] - Q[(old_state, old_move)]))\n",
    "        elif num_moves > 1 and reinf_function is not None:\n",
    "            Q[(old_state, old_move)] += (learning_rate * (reinf_function(state, flappy_box) + Q[state, move] - Q[(old_state, old_move)]))\n",
    "            \n",
    "        # Update moves + states for next move\n",
    "        old_state, old_move = state, move\n",
    "        state = new_state\n",
    "        num_moves += 1\n",
    "\n",
    "        if epsilon_counter % 100 == 0:\n",
    "            print(f'Epsilon: {epsilon}')\n",
    "        y_positions.append(state)\n",
    "\n",
    "        epsilon_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon:  0.999\n",
      "Epsilon:  0.9038873549665959\n",
      "Epsilon:  0.8178301806491574\n",
      "Epsilon:  0.7399663251239436\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 82\u001b[0m, in \u001b[0;36mlearn_game\u001b[0;34m(epsilon_decay, learning_rate, reinf_function)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Update graphics + advance 1 frame\u001b[39;00m\n\u001b[1;32m     81\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_constants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFRAMES_PER_SECOND\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# TODO: Consider adding reinforcement\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Update Q-table of the old (state, move) tuple by calculating the temporal difference error.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Reinforcement of -1\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_moves \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m reinf_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAGdCAYAAAAWp6lMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABE80lEQVR4nO3deViU5eI+8PudYWbYR5BdAQEVRFETFcElTY64VKJZbrnijqZppbao55z6WlqnNBfUMq3MNZc0NY1yZVEQxA3cUFAE3JhBkHXe3x/m/CRNRYGXmbk/1zVXMvPMeD+95ty92yOIoiiCiIiIyIjJpA5AREREVN1YeIiIiMjosfAQERGR0WPhISIiIqPHwkNERERGj4WHiIiIjB4LDxERERk9Fh4iIiIyemZSB6gNdDodsrKyYGNjA0EQpI5DRERET0EUReTn58PNzQ0y2eP34bDwAMjKyoK7u7vUMYiIiOgZZGZmon79+o8dw8IDwMbGBsC9f2G2trYSpyEiIqKnodVq4e7urv8efxwWHkB/GMvW1paFh4iIyMA8zekoPGmZiIiIjB4LDxERERk9Fh4iIiIyeiw8REREZPRYeIiIiMjosfAQERGR0WPhISIiIqPHwkNERERGj4WHiIiIjB4LDxERERk9Fh4iIiIyeiw8REREZPRYeKrZJ7+exrL9F1CuE6WOQkREZLK4Wno1SrmShxUH0wEA0Wdy8cUbLeBubylxKiIiItPDPTzVKKCeGp+9FgArpRxHLt1CjwUHsTEhE6LIvT1EREQ1iYWnGgmCgP5tPLBrcie09rTDneIyvLspBeN+TMTNO8VSxyMiIjIZLDw1wKOuJdaPDcZ73X2hkAv47VQOwr46iOgzOVJHIyIiMgksPDVELhMwoXNDbJnQHo2crHHjTjEiVidg5uYTKCgukzoeERGRUWPhqWHN6qmxfVIHjOrgBQBYeyQDPRceROLlWxInIyIiMl4sPBIwV8jx4cv++Gl0ENzU5rh8sxCvR8Vi/m+pKCnTSR2PiIjI6LDwSCjExwG7pnRC3xfqQScCi/+8gD5LDuNcTr7U0YiIiIwKC4/E1BYK/K9/Sywe1Ap1LBU4laVFr4WH8NnuVNzhuT1ERERVgoWnlujV3BW/TemEzr6OKCnXYem+C+g8fx82HM2EjndpJiIiei6CyLvgQavVQq1WQ6PRwNbWVtIsoihi7+kcfLLzDC7fLAQANHWzxayX/RHkXVfSbERERLVJZb6/WXhQuwrPfcVl5fg+5jIWRp9D/l+HtnoGuGBmjyZcnoKIiAgsPJVWGwvPfTfuFON/e89i3ZEM6ERAaSbDqA5emNClIaxVXAqNiIhMFwtPJdXmwnPfmWtafPzraRw+fxMA4GijwrthvujXqj5kMkHidERERDWPhaeSDKHwAPfO7/n9TC4++fU0Lv11fk+zerb4qBfP7yEiItPDwlNJhlJ47isp02F1zKUK5/f0CnDFjB5+PL+HiIhMBgtPJRla4bnv5p1ifMHze4iIyESx8FSSoRae+1Kztfjvjv9/fo+DtQrvhfmiXyDP7yEiIuPFwlNJhl54gEef38P79xARkTFj4akkYyg89z3q/B7ev4eIiIwRC08lGVPhue/mX/fvWfvA+T0RHbwQyfN7iIjISFTm+9uo1tJavHgxGjRoAHNzcwQFBeHIkSNSR5JMXWsVPukTgJ2TO6J9w7ooKau4Plc51+ciIiITYjSFZ/369Zg6dSpmz56NY8eOoUWLFggLC0Nubq7U0STl52KLHyOCsGJoazSoa4kbd4rx3s8pGP7dEeQVlkgdj4iIqEYYzSGtoKAgtGnTBosWLQIA6HQ6uLu7Y9KkSZgxY8Zj32uMh7Qe5f75Pf/bexZ3S8vhYW+J5UMD4edivHMmIiLjZXKHtEpKSpCYmIjQ0FD9czKZDKGhoYiNjX1ofHFxMbRabYWHKVCayTC6kzd+Hh+C+nYWyLhViL5LYrDzxDWpoxEREVUroyg8N27cQHl5OZydnSs87+zsjOzs7IfGz507F2q1Wv9wd3evqai1gr+bLbZP7ID2DeuisKQcE9Ycw/zfUnleDxERGS2jKDyVNXPmTGg0Gv0jMzNT6kg1zs5KidUj2mJ0Ry8AwOI/L2DU6qPQ3C2VOBkREVHVM4rC4+DgALlcjpycnArP5+TkwMXF5aHxKpUKtra2FR6myEwuwwe9/PFV/5ZQmcnwZ9p1hC8+jHM5+VJHIyIiqlJGUXiUSiUCAwMRHR2tf06n0yE6OhrBwcESJjMM4S/Uw8/jQ1CvjgXSbxQgfPFh/Hbq4UOBREREhsooCg8ATJ06FStWrMDq1atx5swZjB8/HgUFBRgxYoTU0QxCs3pq/DKxPdp526OgpBxjf0jE//aehY7n9RARkREwmlvu9u/fH9evX8esWbOQnZ2Nli1bYvfu3Q+dyEz/rK61Cj9EBOGTX89g1V/LU5zO0uLL/i1gY66QOh4REdEzM5r78DwPU7kPT2VsSryC97ecQEmZDt6OVlg0sBX83fjvhoiIag+Tuw8PVb1+gfWxcWwwXGzNcfF6AXouPIjh3x3B4fM3wI5MRESGhnt4wD08j3M9vxizfzmJXSezcf9PShNXW4zq4IVXWrhBacbOTERE0uBq6ZXEwvNkl28WYOWhdGxIuIK7peUAAGdbFYaFNMDgtp5QW/IcHyIiqlksPJXEwvP08gpLsCY+A6tjLiE3vxgAYKmU443W7hjZ3gsedS0lTkhERKaChaeSWHgqr6RMh+3Hs7Di4EWkZt+7UaFMALr5u2B0Jy8EetpLnJCIiIwdC08lsfA8O1EUcfj8Taw4eBH7z17XP9+xkQMWDngBdlZKCdMREZExY+GpJBaeqnE2Jx/fHLyIrUlZKCnXwbOuJb4d1gYNnayljkZEREaIl6WTJBo722BevxbY8VYH1LezwOWbheiz5DAOnbshdTQiIjJxLDxU5Ro722BbZHu09rRDflEZhn13BGviL0sdi4iITBgLD1WLutYqrBkdhD4v1EO5TsQHW07iP9tPo5xrcxERkQRYeKjaqMzk+N8bLfBOt8YAgJWH0zH6+wTcKS6TOBkREZkaFh6qVoIgYOJLjbBkcCuozGT4IzUX/ZbG4MrtQqmjERGRCWHhoRrRM8AVG8YGw9FGhdTsfIQvPoxjGbeljkVERCaChYdqTAv3OtgW2R5NXG1x404JBiyPwy/Hs6SORUREJoCFh2qUWx0LbBoXjNAmzigp0+GttUn4cu9ZrsBORETVioWHapyVygzLhgRiTCdvAMCC6HN4a10yiv5alJSIiKiqsfCQJOQyAe/3bILPXguAmUzA9uNZGLQiDjfvFEsdjYiIjBALD0mqfxsPfB/RFrbmZjiWkYc+S2JwPveO1LGIiMjIsPCQ5EJ8HLB5Qnu421sg41Yh+i45jJgLXI6CiIiqDgsP1QoNnayxdUJ7tPKoA21RGYZ+ewQbEzKljkVEREaChYdqjbrWKvw0uh1ebu6KMp2Idzel4PPf0qDjchRERPScWHioVjFXyLFwwAuY2KUhAGDRn+fx1rokXsFFRETPhYWHah2ZTMA7Yb6Y3685zGQCdqRcw+Bv4nkFFxERPTMWHqq1Xm/trr+CK/HybV7BRUREz4yFh2q1+1dwedhb8gouIiJ6Ziw8VOs1dLLGlgkhvIKLiIieGQsPGYRHXcHVb2kMdp/MRjmv4iIioicwkzoA0dO6fwWXj6M1luw7j4TLt5FwORGedS0xsr0XXm9dH5ZK/pEmIqKHCSKXqYZWq4VarYZGo4Gtra3Ucegp5GqLsDr2En6My4DmbikAQG2hwOAgDwwLaQBnW3OJExIRUXWrzPc3Cw9YeAxZYUkZNiVewbeH0nH5ZiEAQCEX8GqLehjV0QtNXLk9iYiMFQtPJbHwGL5ynYjfz+Tgm4MXcfTSbf3zHRs5IKKDF15s7AhBECRMSEREVY2Fp5JYeIxLcmYeVhy8iF0nruH++cyNna3x0cv+6NjIUdpwRERUZVh4KomFxzhl3irEqphLWHckAwUl95amGB7SANO7+8FCKZc4HRERPS8Wnkpi4TFu2qJSfPFbGlbHXgYAeDta4av+LdG8fh1pgxER0XOpzPc378NDRs/WXIF/926G1SPbwslGhYvXC9B3SQwW/H4OZeU6qeMREVENYOEhk/FiY0fsebsTev1188Ivfz+LflGxuHid63MRERk7Fh4yKXUslVg08AUsGNASNuZmSM7MQ6+Fh/BD3GXw6C4RkfFi4SGTIwgCeresh9+mdEKIT13cLS3HR1tPYsSqo8jVFkkdj4iIqgELD5kstzoW+DEiCLNe9ofSTIZ9adfR7asD2HnimtTRiIioirHwkEmTyQSM7OCFXyd1QFM3W+QVlmLCmmOYuj4Z2qJSqeMREVEVYeEhAtDI2QZbJrTHxC4NIROAzUlXEb74MDL+Wq6CiIgMGwsP0V+UZjK8E+aLjeOC4aY2v3f5+tLDSM7MkzoaERE9JxYeor8J9LTHlsj28He1xY07JRiwPBZ7TmVLHYuIiJ4DCw/RIzjbmmPDuGB09nVEUakOY39MxHeH06WORUREz4iFh+gfWKvM8M3Q1hgU5AFRBP69/TT+s/00ynW8Xw8RkaFh4SF6DDO5DJ+EN8P07n4AgJWH0zFhTSLu/rUYKRERGQYWHqInEAQB4zv7YOHAF6CUy/DbqRwMXBGHG3eKpY5GRERPiYWH6Cm92sINP44KgtpCgeTMPPRdEsN1uIiIDAQLD1EltPWyx+YJIXC3t0DGrUL0XRqDo5duSR2LiIiegIWHqJJ8HK2xZUJ7tHCvg7zCUgz+Jh7bj2dJHYuIiB6DhYfoGThYq7BudDt083dGSZkOk9YmYdEf56DjFVxERLUSCw/RM7JQyrH0zUCMaN8AAPD5nrMYuvIIV1wnIqqFWHiInoNcJmD2K03x2WsBsFDIcej8DXRfcBC/n86ROhoRET2AhYeoCvRv44HtkzrA39UWtwpKMOr7BMzedhJFpbxfDxFRbcDCQ1RFGjpZY0tkCEZ18AIArI69jN6LDuNsTr7EyYiIiIWHqAqpzOT48GV/rBrRBg7WSqTl5OOVrw/hh7jLEEWe0ExEJBUWHqJq0NnXCbsmd0JnX0cUl+nw0daTGPNDIm4VlEgdjYjIJLHwEFUTRxsVVg5rg49e9odSLsPe0znoseAAYs7fkDoaEZHJYeEhqkYymYCIDl7YEhkCH0cr5GiLMfjbeHy2OxWl5Tqp4xERmQxB5IkF0Gq1UKvV0Gg0sLW1lToOGanCkjL8d8cZrD2SAQBo5GSNl5u7oWsTJzR1s4UgCBInJCIyLJX5/mbhAQsP1axdJ65hxuYT0Nwt1T/nYmuOl5o4oaufE9o3dIC5Qi5hQiIiw8DCU0ksPFTTbheUYO/pHPx+JgcHz93A3Qfu12OukKG9jwO6NnHGS35OcFGbS5iUiKj2YuGpJBYeklJRaTniLt7EH6m5iD6Ti6t5dyu83qyeLV7yc0b3pi7wd+OfTyKi+1h4KomFh2oLURSRmp2PP1Jz8fuZHCRn5uHB/0KHtPPE+z2bwELJQ15ERCw8lcTCQ7XVjTvF+DM1F3tP52DPX+tz+ThaYcGAF9CsnlridERE0mLhqSQWHjIEB89dx7QNx5GbXwyFXMC0br4Y3dEbchmv7iIi01SZ72/eh4fIQHRs5IjfpnRC96YuKC0X8emuVAz+Jg5Zfzvnh4iIHsbCQ2RA7KyUWPpmK8x7rTkslXLEXbyF7l8dwPbjWVJHIyKq1Vh4iAyMIAh4o407dr7VES3d60BbVIZJa5MwdX0y8otKn/wBREQmiIWHyEA1cLDCxnHBeKtrI8gEYHPSVfRYcBAJl25JHY2IqNaptsLzySefICQkBJaWlqhTp84jx2RkZKBXr16wtLSEk5MT3n33XZSVlVUYs2/fPrRq1QoqlQoNGzbEqlWrHvqcxYsXo0GDBjA3N0dQUBCOHDlSDTMiqn0Uchmm/qsxNowNRn07C1y5fRdvLIvFF3vSuFYXEdEDqq3wlJSU4PXXX8f48eMf+Xp5eTl69eqFkpISxMTEYPXq1Vi1ahVmzZqlH5Oeno5evXqhS5cuSE5OxpQpUzBq1Cj89ttv+jHr16/H1KlTMXv2bBw7dgwtWrRAWFgYcnNzq2tqRLVO6wb22DW5I/q2qgedCHz9x3n0i4pF5q1CqaMREdUK1X5Z+qpVqzBlyhTk5eVVeH7Xrl14+eWXkZWVBWdnZwBAVFQUpk+fjuvXr0OpVGL69On49ddfcfLkSf37BgwYgLy8POzevRsAEBQUhDZt2mDRokUAAJ1OB3d3d0yaNAkzZsx4qoy8LJ2MyY6ULLy/+QS0RWWoa6XE8qGtEehpJ3UsIqIqZxCXpcfGxiIgIEBfdgAgLCwMWq0Wp06d0o8JDQ2t8L6wsDDExsYCuLcXKTExscIYmUyG0NBQ/RgiU/NyczfsmtIJTd1scbOgBANXxGFb8lWpYxERSUqywpOdnV2h7ADQ/5ydnf3YMVqtFnfv3sWNGzdQXl7+yDH3P+NRiouLodVqKzyIjEm9OhbYMDYY//J3RkmZDpPXJePLvWfB+4wSkamqVOGZMWMGBEF47CM1NbW6slaZuXPnQq1W6x/u7u5SRyKqclYqMyx7MxBjO3kDABZEn8Nb65JR9MDK7EREpsKsMoOnTZuG4cOHP3aMt7f3U32Wi4vLQ1dT5eTk6F+7/8/7zz04xtbWFhYWFpDL5ZDL5Y8cc/8zHmXmzJmYOnWq/metVsvSQ0ZJJhMws2cTeDta4YMtJ7H9eBau3C7E8iGt4WijkjoeEVGNqVThcXR0hKOjY5X8xsHBwfjkk0+Qm5sLJycnAMDevXtha2sLf39//ZidO3dWeN/evXsRHBwMAFAqlQgMDER0dDTCw8MB3DtpOTo6GhMnTvzH31ulUkGl4l/2ZDr6t/GAu70lxv94DEkZeQhffBgrh7eBr4uN1NGIiGpEtZ3Dk5GRgeTkZGRkZKC8vBzJyclITk7GnTt3AADdunWDv78/hgwZguPHj+O3337Dhx9+iMjISH0ZGTduHC5evIj33nsPqampWLJkCTZs2IC3335b//tMnToVK1aswOrVq3HmzBmMHz8eBQUFGDFiRHVNjcgghfg4YMuEEDSoa4mreXfx2tIY/JnG2zcQkYkQq8mwYcNEAA89/vzzT/2YS5cuiT169BAtLCxEBwcHcdq0aWJpaWmFz/nzzz/Fli1bikqlUvT29ha/++67h36vr7/+WvTw8BCVSqXYtm1bMS4urlJZNRqNCEDUaDTPMlUig3LrTrH4RlSM6Dl9h+g1Y4f43aGLUkciInomlfn+rvb78BgC3oeHTE1JmQ4fbDmBjYlXAABDgz0x62V/mMm52gwRGQ6DuA8PEUlHaSbDvH7NMaOHHwQB+D72MkauToCWi48SkZFi4SEyUYIgYNyLPlg6OBDmChkOnL2OgcvjcKugROpoRERVjoWHyMR1b+aCjWND4GCtxKksLfovi0WutkjqWEREVYqFh4gQUF+NdWOC4WyrwrncO3hjWSyu5t2VOhYRUZVh4SEiAEBDJ2tsHBuC+nYWuHSzEG9ExeLSjQKpYxERVQkWHiLS86hriY3jguHtYIWreXfxxrJYnMvJlzoWEdFzY+Ehogpc1RZYPzYYvs42yM0vRv/lcTiVpZE6FhHRc2HhIaKHONqosG5MOzSvr8atghIMXB6HpIzbUsciInpmLDxE9Eh2Vkr8OCoIrT3toC0qw5vfxCPu4k2pYxERPRMWHiL6R7bmCnwf0RbtG9ZFQUk5hn93BPvPXpc6FhFRpbHwENFjWSrN8O2wNnjJzwlFpTqMXp2APaeypY5FRFQpLDxE9ETmCjmi3gxEzwAXlJTrMH7NMfxyPEvqWERET42Fh4ieitJMhoUDXkDfF+qhXCdi8rokbDiaKXUsIqKnwsJDRE/NTC7D56+3wOAgD4gi8N7PKVhx4KLUsYiInoiFh4gqRSYT8HF4M4zt5A0A+GTnGcz/LRWiKEqcjIjon7HwEFGlCYKAmT2bYHp3PwDA4j8v4IOtJ1GuY+khotqJhYeIntn4zj6Y2zcAggD8FJ+Bt9YloaRMJ3UsIqKHsPAQ0XMZ2NYDiwa2gkIu4NeUa4hYfRSFJWVSxyIiqoCFh4ieW6/mrvh2WBtYKOQ4eO4G3vwmHnmFJVLHIiLSY+EhoirRqbEj1owOgtpCgWMZeei/LA652iKpYxERAWDhIaIq1MrDDhvGBsPJRoW0nHy8FhWDyzcLpI5FRMTCQ0RVy9fFBj+PD4FnXUtk3rqLflGxOHNNK3UsIjJxLDxEVOXc7S2xcVww/FxscD2/GP2XxSLh0i2pYxGRCWPhIaJq4WRjjvVjg9Ha0w7aojK8+W08os/kSB2LiEwUCw8RVRu1hQI/RAShs68jikp1iFidgDe/iUfcxZu8MzMR1SgWHiKqVhZKOVYMbY1hwZ4wkwk4dP4GBiyPw+tRsdiXlsviQ0Q1QhD5tw20Wi3UajU0Gg1sbW2ljkNktK7cLsSy/RexPiFTf0fmgHpqTHypIf7VxBkymSBxQiIyJJX5/mbhAQsPUU3L0RZhxYGLWBOfgbul5QAAX2cbRL7UEL0CXCFn8SGip8DCU0ksPETSuHmnGCsPp+P7mMvIL763HIWXgxUmdPZB+Av1oJDzqDsR/TMWnkpi4SGSluZuKVbHXMLKw+nIKywFANSrY4GJLzXEgDbuEATu8SGih7HwVBILD1HtcKe4DGviLmPFwYu4cefeWlx9XqiHT18LgMpMLnE6IqptKvP9zf3FRFRrWKvMMPZFHxya/hLe7+kHM5mALUlXMeTbI1yMlIieCwsPEdU65go5xnTywaoRbWGjMsOR9Fvou5TrchHRs2PhIaJaq0MjB2waHwI3tTkuXi9AnyUxSLx8W+pYRGSAWHiIqFbzdbHB1sj2CKinxq2CEgxcEYdfU65JHYuIDAwLDxHVek625lg/th1CmzihpEyHyJ+OIWr/Bd6lmYieGgsPERkES6UZlg1pjeEhDQAAn+5KxftbTqKsXCdtMCIyCCw8RGQw5DIBc15titmv+EMQgLVHMjBydQLyi0qljkZEtRwLDxEZnBHtvbB8SGtYKOQ4cPY6Xo+KRVbeXaljEVEtxsJDRAbpX/7O2DA2GI42KqRm56PPksM4eVUjdSwiqqVYeIjIYAXUV2PLhBA0drZGjrYYbyyLxY6ULKljEVEtxMJDRAatvp0lNo0PQYeGDigsKcfEn5Lw0daTKPprFXYiIoCFh4iMgK25AqtGtMGEzj4AgB/iLuO1pTG4dIN3Ziaie1h4iMgomMlleK+7H1aNaAN7KyVOZWnx8teHeIiLiACw8BCRkens64Rf3+qANg3scKe4jIe4iAgACw8RGSFXtQXWjm7HQ1xEpMfCQ0RGiYe4iOhBLDxEZNR4iIuIABYeIjIBPMRFRCw8RGQS/ukQ1+6T2VJHI6IawMJDRCbl74e4xv2YiAW/n4NOJ0odjYiqEQsPEZmc+4e4RrRvAAD48veziPzpGApLyqQNRkTVhoWHiEySmVyG2a80xbzXmkMhF7DrZDZeWxqLK7cLpY5GRNWAhYeITNobbdyxdnQ7OFgrceaaFr0XHcaR9FtSxyKiKsbCQ0Qmr3UDe2yb2AFN3Wxxs6AEg1bEYe2RDKljEVEVYuEhIgJQr44FNo0LQa/mrijTiZi5+QRmbTuJ0nKd1NGIqAqw8BAR/cVCKceigS/g3TBfAMD3sZcx9NsjuF1QInEyInpeLDxERA8QBAGRXRpixdDWsFLKEXvxJl5dfAhp2flSRyOi58DCQ0T0CP/yd8aWyPbwsLdE5q276LvkMPac4k0KiQwVCw8R0T9o7GyDbZHtEeJTFwUl5RjzQyK++v0synmTQiKDw8JDRPQYdlZKrB7ZFsNDGgAAvvr9HAZ/E4dsTZG0wYioUlh4iIieQCGXYc6rTfFl/xawVMoRd/EWeiw4gD9Sc6SORkRPiYWHiOgp9XmhPnZMune/ntuFpRi5KgH/3XEaxWXlUkcjoidg4SEiqgRvR2tsnhCiX4fr20Pp6Lc0FpduFEgbjIgei4WHiKiSVGZyzH6lKVYMbY06lgqcuKpBr4UHsS35qtTRiOgfsPAQET2jf/k7Y9fkjmjbwB4FJeWYvC4Z7248zlXXiWohFh4ioufgqrbAT6ODMLlrI8gEYGPiFbz89SGcztJKHY2IHsDCQ0T0nMzkMrz9r8ZYM6odnG1VuHi9AOFLDuOH2EsQRd6zh6g2YOEhIqoiwT51sWtyJ7zk54SSMh0+2nYKkT8dQ1Epr+IikhoLDxFRFbK3UuLbYa3x0cv+UMgF7DyRjaErj0BbVCp1NCKTxsJDRFTFBEFARAcv/BARBBuVGY6k30L/ZXHIzefdmYmkUm2F59KlS4iIiICXlxcsLCzg4+OD2bNno6SkpMK4lJQUdOzYEebm5nB3d8e8efMe+qyNGzfCz88P5ubmCAgIwM6dOyu8LooiZs2aBVdXV1hYWCA0NBTnzp2rrqkRET2Vdt51sW5sOzhYq3Dmmhb9lsbi8k3er4dICtVWeFJTU6HT6bBs2TKcOnUKX375JaKiovD+++/rx2i1WnTr1g2enp5ITEzE/PnzMWfOHCxfvlw/JiYmBgMHDkRERASSkpIQHh6O8PBwnDx5Uj9m3rx5WLhwIaKiohAfHw8rKyuEhYWhqIj/N0VE0mrqpsbP44PhYW+JjFuFeG1pLE5laaSORWRyBLEGLyGYP38+li5diosXLwIAli5dig8++ADZ2dlQKpUAgBkzZmDr1q1ITU0FAPTv3x8FBQXYsWOH/nPatWuHli1bIioqCqIows3NDdOmTcM777wDANBoNHB2dsaqVaswYMCAJ+bSarVQq9XQaDSwtbWt6mkTESFXW4ShK48gNTsfNiozfDOsNYK860odi8igVeb7u0bP4dFoNLC3t9f/HBsbi06dOunLDgCEhYUhLS0Nt2/f1o8JDQ2t8DlhYWGIjY0FAKSnpyM7O7vCGLVajaCgIP2YvysuLoZWq63wICKqTk625lg/NhhtG9gjv7gMQ1YewZ5T2VLHIjIZNVZ4zp8/j6+//hpjx47VP5ednQ1nZ+cK4+7/nJ2d/dgxD77+4PseNebv5s6dC7VarX+4u7s/x8yIiJ6O2kKB7yPaIrSJM0rKdBj3YyI2HM2UOhaRSah04ZkxYwYEQXjs4/7hqPuuXr2K7t274/XXX8fo0aOrLPyzmjlzJjQajf6Rmcm/cIioZpgr5Ih6sxVeD6wPnQi893MKovZf4A0KiaqZWWXfMG3aNAwfPvyxY7y9vfW/zsrKQpcuXRASElLhZGQAcHFxQU5OToXn7v/s4uLy2DEPvn7/OVdX1wpjWrZs+ch8KpUKKpXqsXMgIqouZnIZ5vVrDntrJZbtv4hPd6Xi5p1izOzRBDKZIHU8IqNU6cLj6OgIR0fHpxp79epVdOnSBYGBgfjuu+8gk1XcoRQcHIwPPvgApaWlUCgUAIC9e/fC19cXdnZ2+jHR0dGYMmWK/n179+5FcHAwAMDLywsuLi6Ijo7WFxytVov4+HiMHz++stMjIqoRgiBgZo8mqGulxP/tTMWKg+m4VVCKT18LgELOW6QRVbVq+6/q6tWr6Ny5Mzw8PPD555/j+vXryM7OrnBezaBBg6BUKhEREYFTp05h/fr1WLBgAaZOnaofM3nyZOzevRtffPEFUlNTMWfOHCQkJGDixIkA7v2lMWXKFHz88cf45ZdfcOLECQwdOhRubm4IDw+vrukREVWJMZ188PnrLSCXCfj52BWM+T4BtwtKnvxGIqqUarssfdWqVRgxYsQjX3vwt0xJSUFkZCSOHj0KBwcHTJo0CdOnT68wfuPGjfjwww9x6dIlNGrUCPPmzUPPnj0rfN7s2bOxfPly5OXloUOHDliyZAkaN278VFl5WToRSS36TA4mrDmG4jIdHKxV+KRPM4Q1dZE6FlGtVpnv7xq9D09txcJDRLXBiSsaTN2QjHO5dwAAvVu6Yc4rTWFnpXzCO4lMU629Dw8REf2zgPpqbJ/UAeM7+0AmANuSs9DtqwPYezrnyW8mosdi4SEiqkXMFXJM7+6HzRPaw8fRCtfzizH6+wS8vT4ZeYU8t4foWbHwEBHVQi3d6+DXtzpi7IvekAnAlqSr6PblAUSf4d4eomfBwkNEVEuZK+SY2aMJNo0PgbejFXLzixGxOgFTNyRDU1gqdTwig8LCQ0RUy7XysMPOtzpiTCdvCAKw+dhVdPtqP/5MzZU6GpHBYOEhIjIA5go53u/ZBJvGBcPbwQo52mKMWHUU7248jsKSMqnjEdV6LDxERAYk0NMeOyd3xKgOXhAEYGPiFQxaEY9bvFkh0WOx8BARGRhzhRwfvuyPtaPboY6lAsmZeegXFYPMW4VSRyOqtVh4iIgMVDvvutg0Lhj16ljg4vUCvLY0BqeztFLHIqqVWHiIiAxYQycb/Dw+BH4uNsjNL0b/ZbGIuXBD6lhEtQ4LDxGRgXNRm2P92GAEedkjv7gMw1cexa8p16SORVSrsPAQERkBtYUCq0e2RY9mLigp12Hi2mNYdThd6lhEtQYLDxGRkTBXyLFoUCsMDfaEKAJztp/GvN2p4BrRRCw8RERGRS4T8O9Xm+LdMF8AwJJ9F/DOxhSUluskTkYkLRYeIiIjIwgCIrs0xLzXmkMuE/DzsSsY/X0Cb1BIJo2Fh4jISL3Rxh0rhgbCXCHDvrTrGLgiHjfvFEsdi0gSLDxEREbsJT9n/PTXDQqPZ+ahX1QsMm7yBoVkelh4iIiMXCsPO2waF4J6dSyQfqMA4UsO4+ilW1LHIqpRLDxERCagoZM1Nk8IQUA9NW4VlGDwinhsPnZF6lhENYaFh4jIRDjbmmPD2GB0b3rvXj1TNxzH57+lQafjZetk/Fh4iIhMiIVSjiWDW2FCZx8AwKI/z2Pi2mO4W1IucTKi6sXCQ0RkYmQyAe9198Pnr7eAQi5g54ls9F8ei1xtkdTRiKoNCw8RkYnqF1gfP0YEwc5SgZQrGvRefBinsjRSxyKqFiw8REQmLMi7LrZGtoePoxWuaYrwelQs9p7OkToWUZVj4SEiMnGeda2weUJ7dGjogMKScoz5IQHLD1zgGlxkVFh4iIgIagsFvhvRBoODPCCKwP/tTMWMn0+gpIxrcJFxYOEhIiIAgEIuw8fhzTD7FX/IBGB9QiaGroxHXmGJ1NGInhsLDxER6QmCgBHtvfDNsNawUsoRd/EWwhcfxvncO1JHI3ouLDxERPSQl/yc8fOEe8tRXLpZiD5LDmNfWq7UsYieGQsPERE9kp+LLbZNbI/WnnbILyrDyFVHsfJQOk9mJoPEwkNERP/IwVqFNaOD8HpgfehE4D87TmPmZp7MTIaHhYeIiB5LZSbHvH7N8UHPJhAEYN3RTLz5bTxuFfBkZjIcLDxERPREgiBgdCdvrBzWBtYqMxxJv4Xeiw/hbE6+1NGIngoLDxERPbUufk7YPCEEHvaWyLx1F32XxCD6DO/MTLUfCw8REVVKY2cbbI1sjyAve9wpLsOo73lnZqr9WHiIiKjS7K2U+CEiCAPbuuvvzPzuphQUl5VLHY3okVh4iIjomSjNZPi/PgH6OzNvSryCQSviceNOsdTRiB7CwkNERM/s/p2ZV41oCxtzMyRevo3XlsYgV1skdTSiClh4iIjouXVq7IgtE9qjvp0FLt8sxOBveNk61S4sPEREVCUaOllj7eh2cLE1x7ncOxi6Mh7aolKpYxEBYOEhIqIq5G5viR9HBaGulRInr2ox8rujKCwpkzoWEQsPERFVrYZO1vg+oi1szc2QcPk2xnyfiKJSXr1F0mLhISKiKtfUTY1VI9vCUinHofM3MPGnYygt5/pbJB0WHiIiqhatPOzw7bA2UJnJ8PuZXEzdcBzlOt6ckKTBwkNERNUm2Kcuot4MhEIuYPvxLMzcnAIdSw9JgIWHiIiqVRc/JywY8AJkArAh4Qr+s+M0l6GgGsfCQ0RE1a5ngCvm9WsBAFgVcwlf7DkrcSIyNSw8RERUI/oF1sd/ezcFACz68zyW7DsvcSIyJSw8RERUY4YEN8CMHn4AgHm707A65pK0gchksPAQEVGNGveiD956qSEAYPYvp7DhaKbEicgUsPAQEVGNe/tfjRHRwQsA8N7PKZjxcwryuQwFVSMWHiIiqnGCIODDXk0w9kVvAMC6o5kI+/IADp67LnEyMlYsPEREJAlBEDCzRxOsH9MOHvaWyNIUYci3RzBz8wncKeb6W1S1WHiIiEhSQd51sXtKRwwPaQAAWHskA2FfHsDh8zekDUZGhYWHiIgkZ6k0w5xXm2Lt6Haob2eBq3l3MfibeHywhXt7qGqw8BARUa0R7FMXv03phCHtPAEAa+Iz0P2rA4i5wL099HxYeIiIqFaxUpnhv+HN8NOoINSrY4Ert+9i0Ip4zNp2EgXc20PPiIWHiIhqpZCGDvjt7U4YHOQBAPg+9jK6LziAuIs3JU5GhoiFh4iIai1rlRk+6ROAHyPu7e3JvHUXA1fE4bPdqSgt10kdjwwICw8REdV6HRo5YPeUjhjQxh2iCCzddwFvLItF5q1CqaORgWDhISIig2BjrsCnrzXHksGtYGNuhqSMPPRceBC7TlyTOhoZABYeIiIyKD0DXLHzrY54waMO8ovKMH7NMXyw5QSKSsuljka1GAsPEREZHHd7S2wYG4zxnX0A3Lt8vfeiwziXky9xMqqtWHiIiMggKeQyTO/uh+9HtoWDtRJpOfl4ZdEhrD+aAVEUpY5HtQwLDxERGbROjR2xc3JHdGzkgKJSHab/fAJvrUuGlquv0wNYeIiIyOA52Zhj9Yi2mN7dD3KZgO3Hs/DywkM4npkndTSqJVh4iIjIKMhkAsZ39sGGscGoV8cCGbcK8drSGKw4cBE6HQ9xmToWHiIiMiqBnnbYObkjejRzQZlOxCc7z2DwN/G8Z4+JY+EhIiKjo7ZQYMngVvikTzNYKOSIvXgTYV8dwA+xl7i3x0Sx8BARkVESBAGDgzyxe0pHtPWyR2FJOT7adop7e0wUCw8RERk1z7pWWDe6Hea84l9xb0/cZe7tMSHVWnheffVVeHh4wNzcHK6urhgyZAiysrIqjElJSUHHjh1hbm4Od3d3zJs376HP2bhxI/z8/GBubo6AgADs3LmzwuuiKGLWrFlwdXWFhYUFQkNDce7cueqcGhERGRCZTMDw9l739vY0+Gtvz9aT3NtjQqq18HTp0gUbNmxAWloafv75Z1y4cAH9+vXTv67VatGtWzd4enoiMTER8+fPx5w5c7B8+XL9mJiYGAwcOBARERFISkpCeHg4wsPDcfLkSf2YefPmYeHChYiKikJ8fDysrKwQFhaGoqKi6pweEREZGM+6Vlg3ph1mv+IPc4WMe3tMiCDW4O0of/nlF4SHh6O4uBgKhQJLly7FBx98gOzsbCiVSgDAjBkzsHXrVqSmpgIA+vfvj4KCAuzYsUP/Oe3atUPLli0RFRUFURTh5uaGadOm4Z133gEAaDQaODs7Y9WqVRgwYMATc2m1WqjVamg0Gtja2lbDzImIqLa5dKMA721KwZFLtwAAwd51Ma9fc7jbW0qcjJ5WZb6/a+wcnlu3bmHNmjUICQmBQqEAAMTGxqJTp076sgMAYWFhSEtLw+3bt/VjQkNDK3xWWFgYYmNjAQDp6enIzs6uMEatViMoKEg/5u+Ki4uh1WorPIiIyLQ0cODeHlNS7YVn+vTpsLKyQt26dZGRkYFt27bpX8vOzoazs3OF8fd/zs7OfuyYB19/8H2PGvN3c+fOhVqt1j/c3d2fY4ZERGSoZDIBI9p7YffkThXO7Zm0LgklZTqp41EVqnThmTFjBgRBeOzj/uEoAHj33XeRlJSEPXv2QC6XY+jQoZIv6jZz5kxoNBr9IzMzU9I8REQkrft7e2a97A+FXMCvKdcw7sdEFJWWSx2NqohZZd8wbdo0DB8+/LFjvL299b92cHCAg4MDGjdujCZNmsDd3R1xcXEIDg6Gi4sLcnJyKrz3/s8uLi76fz5qzIOv33/O1dW1wpiWLVs+Mp9KpYJKpXryZImIyGTIZAJGdvCCt6MVxv6QiD9SczFy1VGsGNoaVqpKf11SLVPpPTyOjo7w8/N77OPBc3IepNPd2z1YXFwMAAgODsaBAwdQWvr/V7Tdu3cvfH19YWdnpx8THR1d4XP27t2L4OBgAICXlxdcXFwqjNFqtYiPj9ePISIielqdfZ2wemRbWCnliLlwE0NXHoHmLldeN3TVdg5PfHw8Fi1ahOTkZFy+fBl//PEHBg4cCB8fH30RGTRoEJRKJSIiInDq1CmsX78eCxYswNSpU/WfM3nyZOzevRtffPEFUlNTMWfOHCQkJGDixIkA7t1Jc8qUKfj444/xyy+/4MSJExg6dCjc3NwQHh5eXdMjIiIj1s67Ln4cFQRbczMkXr6NQSvicKugROpY9DzEapKSkiJ26dJFtLe3F1UqldigQQNx3Lhx4pUrVyqMO378uNihQwdRpVKJ9erVEz/99NOHPmvDhg1i48aNRaVSKTZt2lT89ddfK7yu0+nEjz76SHR2dhZVKpXYtWtXMS0t7amzajQaEYCo0WiebbJERGSUTl3ViK3+s0f0nL5DDP1in5ijuSt1JHpAZb6/a/Q+PLUV78NDRET/5HzuHbz5TTyytUXwrGuJNaOCUN+O9+qpDWrlfXiIiIgMUUMna2wcFwx3ewtcvlmIN6JicfH6HaljUSWx8BARET2Bu70lNo4NgY+jFbI0RXhjWRzSsvOljkWVwMJDRET0FFzU5lg/NhhNXG1x404x+i+PRcqVPKlj0VNi4SEiInpKDtYqrBvdDi3d6yCvsBSDVsTj6F9rcVHtxsJDRERUCWpLBX4cFYQgL3vcKS7D0G+P4MDZ61LHoidg4SEiIqoka5UZVo1oixcbO+JuaTlGrDqKbw5elHzpJPpnLDxERETPwEIpx/Khgejbqh7KdSI+/vUMJq1NQmFJmdTR6BFYeIiIiJ6RykyOL15vgf/0bgozmYAdKdfQZ3EMLt0okDoa/Q0LDxER0XMQBAFDgxtg3Zh2cLRRIS0nH68sOoToMzlPfjPVGBYeIiKiKtC6gT12TOqAQE875BeVIWJ1Ar7cexY6Hc/rqQ1YeIiIiKqIs6051o5uh6HBngCABdHnMOr7BK62Xguw8BAREVUhpZkM/+ndDJ+/3gIqMxn+SM3Fq4sOITVbK3U0k8bCQ0REVA36BdbHz+NDUK/OvTW4+iyOwS/Hs6SOZbJYeIiIiKpJs3pq7JjUAR0bOeBuaTneWpuET349jbJyndTRTA4LDxERUTWys1Ji1Yi2GN/ZBwCw4mA63vw2Hrn5RRInMy0sPERERNVMLhMwvbsflg5uBSulHHEXb6HXwkOIu3hT6mgmg4WHiIiohvQIcMW2iR3Q2Nka1/OLMWhFHJbuu8BL12sACw8REVENauhkja2R7dH3hXrQicBnu1Mx+vsEaAp56Xp1YuEhIiKqYZZKM3zxRgvM7RsApZkM0am56PX1QaRcyZM6mtFi4SEiIpKAIAgY2NYDm8eHwMPeEldu30W/pbH4Ie4yV12vBiw8REREEmpWT43tkzqgm78zSsp1+GjrSUxel4yCYq66XpVYeIiIiCSmtlBg2ZBAfNirCeQyAb8cz8Kriw7hbE6+1NGMBgsPERFRLSAIAkZ19Ma6Me3gbKvChesF6L3oMLYkXZE6mlFg4SEiIqpF2jSwx69vdUSHhvfuzvz2+uOYuTkF2iJexfU8WHiIiIhqGQdrFVaPbIu3ujaCIABrj2Tipc/3YUNCJu/Z84xYeIiIiGohuUzA1H81xo8RQfB2sMKNOyV4b1MK+iw5jKSM21LHMziCyGvfoNVqoVarodFoYGtrK3UcIiKiCkrKdFgdcwkLos/hzl9Xb73Wqj6m9/CFk425xOmkU5nvbxYesPAQEZFhyM0vwrzdadiUeO9EZmuVGd7q2hDDQ7ygNDO9gzYsPJXEwkNERIYkKeM25mw/jeOZeQAAbwcrfPSKP7r4OkkbrIax8FQSCw8RERkanU7Ez8eu4LPdabhxpxgA0NXPCR+97I8GDlYSp6sZLDyVxMJDRESGKr+oFF//cR4rD6WjTCdCKZdhZAcvTAltBHOFXOp41aoy39+md8CPiIjIiNiYK/B+zybYPaUTXmzsiJJyHaL2X0D/5XHI1RZJHa/WYOEhIiIyAg2drLFqRBusGNoaagsFjmfmoffiwzh5VSN1tFqBhYeIiMhICIKAf/k7Y1tke/g4WuGapgivR8Vi14lrUkeTHAsPERGRkWngYIXNE9qjU2NH3C0tx/g1x/B19DmY8mm7LDxERERGSG2hwMphrTGifQMAwBd7z2LyumQUlZZLG0wiLDxERERGykwuw+xXmuL/+gTATCbgl+NZ6L8s1iRPZmbhISIiMnKDgjzwQ0QQ6lgqcPyKBq8uMr2TmVl4iIiITECwT11sndAeDZ2ska0tQr+oGOw0oZOZWXiIiIhMxL2TmUPwYmNHFJXqMGHNMSz43TROZmbhISIiMiG25gp8O6w1Rrb3AgB8+ftZTFqbhLslxn0yMwsPERGRiTGTyzDrFX982vfeycw7Uq6h9+JDSM3WSh2t2rDwEBERmagBbT3w46ggOFircDbnDl5ddBirYy4Z5SEuFh4iIiIT1s67LnZP6Yguvo4oKdNh9i+nMPr7BNwqKJE6WpVi4SEiIjJxDtYqrBzeBrNf8YdSLsPvZ3LR/asDOHTuhtTRqgwLDxEREUEQBIxo74WtkfcuXc/NL8aQlfGYu+sMSsp0Usd7biw8REREpOfvZovtEztgUJAHRBFYtv8i+kXF4NKNAqmjPRcWHiIiIqrAQinH//UJQNSbraC2UCDliga9Fh7EpsQrBntCMwsPERERPVL3Zq7YPaUjgrzsUVBSjnc2HsfkdcnQFpVKHa3SWHiIiIjoH7mqLfDT6HZ4N8wX8r8WIO254CASL9+WOlqlsPAQERHRY8llAiK7NMSmccFwt7fAldt38cayWCzZdx46nWEc4mLhISIioqfygocddr7VEb1buqFcJ2Le7jQMXXkEuflFUkd7IhYeIiIiemo25gp81b8l5vVrDguFHIfO30DPBQex/+x1qaM9FgsPERERVYogCHijtTu2T2oPPxcb3LhTgmErj2DurjMoLa+d9+xh4SEiIqJn0tDJBlsj22NIO08A9+7Z83pULDJvFUqc7GEsPERERPTMzBVy/De8GaLebAVbczMkZ+ah54KD+DXlmtTRKmDhISIioufWvZkrdk7uiFYedZBfXIbIn45h5uYTuFtSLnU0ACw8REREVEXq21li/dhgRHbxgSAAa49koPfiQzibky91NBYeIiIiqjoKuQzvhvnhh5FBcLRR4WzOHby66BDWHsmQdFkKFh4iIiKqch0aOWDnWx3RqbEjikp1+HDrSVyUcAFSM8l+ZyIiIjJqjjYqrBreBisOXoQgAD6O1pJlYeEhIiKiaiOTCRj7oo/UMXhIi4iIiIwfCw8REREZPRYeIiIiMnosPERERGT0WHiIiIjI6LHwEBERkdFj4SEiIiKjx8JDRERERq9GCk9xcTFatmwJQRCQnJxc4bWUlBR07NgR5ubmcHd3x7x58x56/8aNG+Hn5wdzc3MEBARg586dFV4XRRGzZs2Cq6srLCwsEBoainPnzlXnlIiIiMiA1Ejhee+99+Dm5vbQ81qtFt26dYOnpycSExMxf/58zJkzB8uXL9ePiYmJwcCBAxEREYGkpCSEh4cjPDwcJ0+e1I+ZN28eFi5ciKioKMTHx8PKygphYWEoKiqqiekRERFRbSdWs507d4p+fn7iqVOnRABiUlKS/rUlS5aIdnZ2YnFxsf656dOni76+vvqf33jjDbFXr14VPjMoKEgcO3asKIqiqNPpRBcXF3H+/Pn61/Py8kSVSiWuXbv2qTJqNBoRgKjRaJ5likRERCSBynx/V+senpycHIwePRo//PADLC0tH3o9NjYWnTp1glKp1D8XFhaGtLQ03L59Wz8mNDS0wvvCwsIQGxsLAEhPT0d2dnaFMWq1GkFBQfoxf1dcXAytVlvhQURERMar2gqPKIoYPnw4xo0bh9atWz9yTHZ2NpydnSs8d//n7Ozsx4558PUH3/eoMX83d+5cqNVq/cPd3b2SsyMiIiJDUunV0mfMmIHPPvvssWPOnDmDPXv2ID8/HzNnznzmcNVl5syZmDp1qv5njUYDDw8P7ukhIiIyIPe/t0VRfOLYSheeadOmYfjw4Y8d4+3tjT/++AOxsbFQqVQVXmvdujUGDx6M1atXw8XFBTk5ORVev/+zi4uL/p+PGvPg6/efc3V1rTCmZcuWj8ynUqkq5Lr/L4x7eoiIiAxPfn4+1Gr1Y8dUuvA4OjrC0dHxieMWLlyIjz/+WP9zVlYWwsLCsH79egQFBQEAgoOD8cEHH6C0tBQKhQIAsHfvXvj6+sLOzk4/Jjo6GlOmTNF/1t69exEcHAwA8PLygouLC6Kjo/UFR6vVIj4+HuPHj3+qObm5uSEzMxM2NjYQBOGp3vO0tFot3N3dkZmZCVtb2yr97NqKc+acjRXnzDkbK0OdsyiKyM/Pf+SV4I8aXCPS09MfukorLy9PdHZ2FocMGSKePHlSXLdunWhpaSkuW7ZMP+bw4cOimZmZ+Pnnn4tnzpwRZ8+eLSoUCvHEiRP6MZ9++qlYp04dcdu2bWJKSorYu3dv0cvLS7x7925NTe8fmeIVYJyzaeCcTQPnbBpMYc6V3sNTldRqNfbs2YPIyEgEBgbCwcEBs2bNwpgxY/RjQkJC8NNPP+HDDz/E+++/j0aNGmHr1q1o1qyZfsx7772HgoICjBkzBnl5eejQoQN2794Nc3NzKaZFREREtYwgik9xpg89M61WC7VaDY1GY1C7CZ8H58w5GyvOmXM2VqYwZ66lVc1UKhVmz5790MnbxoxzNg2cs2ngnE2DKcyZe3iIiIjI6HEPDxERERk9Fh4iIiIyeiw8REREZPRYeIiIiMjosfBUs8WLF6NBgwYwNzdHUFAQjhw5InWkajNnzhwIglDh4efnJ3WsKnXgwAG88sorcHNzgyAI2Lp1a4XXRVHErFmz4OrqCgsLC4SGhuLcuXPShK0iT5rz8OHDH9ru3bt3lyZsFZg7dy7atGkDGxsbODk5ITw8HGlpaRXGFBUVITIyEnXr1oW1tTVee+21h5bAMSRPM+fOnTs/tJ3HjRsnUeLnt3TpUjRv3hy2trawtbVFcHAwdu3apX/d2LYx8OQ5G9s2/jsWnmq0fv16TJ06FbNnz8axY8fQokULhIWFITc3V+po1aZp06a4du2a/nHo0CGpI1WpgoICtGjRAosXL37k6/PmzcPChQsRFRWF+Ph4WFlZISwsDEVFRTWctOo8ac4A0L179wrbfe3atTWYsGrt378fkZGRiIuLw969e1FaWopu3bqhoKBAP+btt9/G9u3bsXHjRuzfvx9ZWVno27evhKmfz9PMGQBGjx5dYTvPmzdPosTPr379+vj000+RmJiIhIQEvPTSS+jduzdOnToFwPi2MfDkOQPGtY0fIul9no1c27ZtxcjISP3P5eXlopubmzh37lwJU1Wf2bNniy1atJA6Ro0BIG7ZskX/s06nE11cXMT58+frn8vLyxNVKpW4du1aCRJWvb/PWRRFcdiwYWLv3r0lyVMTcnNzRQDi/v37RVG8t00VCoW4ceNG/ZgzZ86IAMTY2FipYlapv89ZFEXxxRdfFCdPnixdqBpgZ2cnfvPNNyaxje+7P2dRNP5tzD081aSkpASJiYkIDQ3VPyeTyRAaGorY2FgJk1Wvc+fOwc3NDd7e3hg8eDAyMjKkjlRj0tPTkZ2dXWGbq9VqBAUFGfU2B4B9+/bByckJvr6+GD9+PG7evCl1pCqj0WgAAPb29gCAxMRElJaWVtjOfn5+8PDwMJrt/Pc537dmzRo4ODigWbNmmDlzJgoLC6WIV+XKy8uxbt06FBQUIDg42CS28d/nfJ+xbmPgGVZLp6dz48YNlJeXw9nZucLzzs7OSE1NlShV9QoKCsKqVavg6+uLa9eu4d///jc6duyIkydPwsbGRup41S47OxsAHrnN779mjLp3746+ffvCy8sLFy5cwPvvv48ePXogNjYWcrlc6njPRafTYcqUKWjfvr1+/b7s7GwolUrUqVOnwlhj2c6PmjMADBo0CJ6ennBzc0NKSgqmT5+OtLQ0bN68WcK0z+fEiRMIDg5GUVERrK2tsWXLFvj7+yM5Odlot/E/zRkwzm38IBYeqjI9evTQ/7p58+YICgqCp6cnNmzYgIiICAmTUXUaMGCA/tcBAQFo3rw5fHx8sG/fPnTt2lXCZM8vMjISJ0+eNLpz0R7nn+b84KLOAQEBcHV1RdeuXXHhwgX4+PjUdMwq4evri+TkZGg0GmzatAnDhg3D/v37pY5Vrf5pzv7+/ka5jR/EQ1rVxMHBAXK5/KGz+nNycuDi4iJRqppVp04dNG7cGOfPn5c6So24v11NeZsDgLe3NxwcHAx+u0+cOBE7duzAn3/+ifr16+ufd3FxQUlJCfLy8iqMN4bt/E9zfpSgoCAAMOjtrFQq0bBhQwQGBmLu3Llo0aIFFixYYNTb+J/m/CjGsI0fxMJTTZRKJQIDAxEdHa1/TqfTITo6usLxUmN2584dXLhwAa6urlJHqRFeXl5wcXGpsM21Wi3i4+NNZpsDwJUrV3Dz5k2D3e6iKGLixInYsmUL/vjjD3h5eVV4PTAwEAqFosJ2TktLQ0ZGhsFu5yfN+VGSk5MBwGC386PodDoUFxcb5Tb+J/fn/ChGt42lPmvamK1bt05UqVTiqlWrxNOnT4tjxowR69SpI2ZnZ0sdrVpMmzZN3Ldvn5ieni4ePnxYDA0NFR0cHMTc3Fypo1WZ/Px8MSkpSUxKShIBiP/73//EpKQk8fLly6IoiuKnn34q1qlTR9y2bZuYkpIi9u7dW/Ty8hLv3r0rcfJn97g55+fni++8844YGxsrpqeni7///rvYqlUrsVGjRmJRUZHU0Z/J+PHjRbVaLe7bt0+8du2a/lFYWKgfM27cONHDw0P8448/xISEBDE4OFgMDg6WMPXzedKcz58/L/7nP/8RExISxPT0dHHbtm2it7e32KlTJ4mTP7sZM2aI+/fvF9PT08WUlBRxxowZoiAI4p49e0RRNL5tLIqPn7MxbuO/Y+GpZl9//bXo4eEhKpVKsW3btmJcXJzUkapN//79RVdXV1GpVIr16tUT+/fvL54/f17qWFXqzz//FAE89Bg2bJgoivcuTf/oo49EZ2dnUaVSiV27dhXT0tKkDf2cHjfnwsJCsVu3bqKjo6OoUChET09PcfTo0QZd6h81VwDid999px9z9+5dccKECaKdnZ1oaWkp9unTR7x27Zp0oZ/Tk+ackZEhdurUSbS3txdVKpXYsGFD8d133xU1Go20wZ/DyJEjRU9PT1GpVIqOjo5i165d9WVHFI1vG4vi4+dsjNv47wRRFMWa259EREREVPN4Dg8REREZPRYeIiIiMnosPERERGT0WHiIiIjI6LHwEBERkdFj4SEiIiKjx8JDRERERo+Fh4iIiIweCw8REREZPRYeIiIiMnosPERERGT0WHiIiIjI6P0/9gWQg2F8fmAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_positions[0::10])\n",
    "print(len(y_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definitely see an improvement over time. Ideally, we want to be around 250, as this is the middle of the gap between the two walls. Toward the beginning, the box is at 0 (top of the screen) the majority of the time, as multiple '0' actions are required to fall, while only one '1' action (jump) is needed to push back toward the top of the screen. However, the box eventually learns that multiple falling actions might be a good to escape the ceiling and head toward the gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two real parameters to change here: the learning rate and epsilon decay. We'll try two radically different values for both, one higher and one lower, and examine the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing epsilon decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon:  0.99\n",
      "Epsilon:  0.36237201786049694\n",
      "Epsilon:  0.13263987810938213\n",
      "Epsilon:  0.0485504851305729\n"
     ]
    }
   ],
   "source": [
    "y_positions = []\n",
    "learn_game(epsilon_decay=0.99) # Epsilon decays much quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_positions[0::10])\n",
    "plt.title('Positions over time with epsilon = 0.99')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this epsilon decay value brought epsilon down too quickly. The box seemed to learn that hugging the top of the screen wasn't optimal, but it very infrequently came between the gap in the wall. This is presumably because it didn't explore enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_positions = []\n",
    "epsilon_decay = 0.9999\n",
    "learn_game(epsilon_decay=0.9999) # Epsilon decays much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_positions[0::10])\n",
    "plt.title(f'Positions over time with epsilon = {epsilon_decay}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 2 minutes, epsilon was not able to decay to a value even close to 0. Because of this, it was frequently picking a random move even at the end, which more often than not caused the box to stay toward the top of the screen. This result could be interesting however; let's pick an epsilon in the middle and increase the time to 5 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_positions = []\n",
    "epsilon_decay = 0.9995\n",
    "learn_game(epsilon_decay=epsilon_decay) # Epsilon decays a bit slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_positions[0::10])\n",
    "plt.title(f'Positions over time with epsilon = {epsilon_decay}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toward the very end, the agent seemed to be performing at its best compared to other epsilon values. However, we did also train this for 5 minutes vs. 2 minutes. If anything, this just shows that longer training times might be necessary for optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't expect changing the learning rate will have as big of an effect as changing the epsilon decay, as the Q-values being altered are very large. However, we will see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_positions = []\n",
    "learning_rate = 0.3\n",
    "learn_game(learning_rate=learning_rate) # Learn less from the immediate future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_positions[0::10])\n",
    "plt.title(f'Positions over time with learning rate = {learning_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to my prediction, this actually seemed to have a substantial effect, as we can see from the graph. The box tended to hover around 250 a good amount of time toward the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_positions = []\n",
    "learning_rate = 1.3\n",
    "learn_game(learning_rate=learning_rate) # \"Learn\" more from the immediate future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_positions[0::10])\n",
    "plt.title(f'Positions over time with learning rate = {learning_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This learning rate seemed to make the agent perform much worse. It was giving too much weight in the temporal difference formula. We do not see the agent come close to converging even after two minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Reinforcement Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we were not that concerned with what the box was doing when it was far away from the walls; we only cared about what happened when it was immediately around the walls. However, a simple reinforcement function might help the box stay near the center, such that it might converge quicker on good Q-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the box is in the top half of the screen, favor positive velocities\n",
    "# Else, it's in the bottom half, so we should favor negative velocities\n",
    "def reinforcement2(state, box):\n",
    "    if state > 250:  # Bottom half, favor negative velocities\n",
    "        return box.y_velocity * -1\n",
    "    else:  # Top half\n",
    "        return box.y_velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might think that reinforcing based off of position would be a simple idea. However, this turned out to be more complicated than initially thought. If the box is in the top half and travelling on an upward arc, a move of 0 (doing nothing) would be desired. Yet all the algorithm would see is a \"nothing\" move that moved the box upward. Because of this, favorable moves might be given bad values, so velocity was chosen instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_positions = []\n",
    "learn_game(reinf_function=reinforcement2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_positions[0::10])\n",
    "plt.title('Positions over time with velocity reinforcement function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, adding the reinforcement function caused our agent to perform the best. We can see a definitive progression toward 250. But most importantly, once the agent learned this was a good place to be, it didn't seem to regress back toward constantly hugging the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two parameters seemed to have a large effect on the performance of the agent, although the default parameters performed at an acceptable level. The default value of epsilon seemed to perform best at the two minute mark, although I have little doubt that epsilon = 0.9999 would outclass this given noticeably longer times (10+ minutes). The agent also seemed to respond well to a lower-than-default learning rate, where the Q-values were updated in a less extreme manner. Finally, the simple reinforcement function I've implemented seemed to help more than not, and gave the agent an idea for where to aim even when near no walls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Challenges Faced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I would say the assignment was a success. It was a great way to apply machine learning concepts to a relevant example. It taught me how to look for such abstract concepts as \"state\" and \"action\" when they weren't explicitly defined.\n",
    "\n",
    "The main challenge of the assignment was not the machine learning; it was working with Pygame. I've never coded a game in it before, so I wasn't really sure how to translate my thoughts to code. Luckily, Pygame has good documentation and the common questions that I needed had been answered before. Moving to the machine learning side, the main issue I faced was debugging. When the game wasn't working as desired, it was hard to tell if it was my fault in the game logic, the algorithm, or how I had defined Q-learning. The only real means I had of examining it was to watch the game unfold on screen.\n",
    "\n",
    "Moving forward, I would like to get the reinforcement learning working with random walls, and perhaps implement a better reinforcement function such that the solution might converge faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
